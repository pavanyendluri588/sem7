library("cluster")
library(cluster)
data = iris
data1 = data[-5]
kmeans(data1,centers = 3,nstart = 100)
kmeans_model = kmeans(data1,centers = 3,nstart = 100)
kmeans_model = kmeans(data1,centers = 3,nstart = 100)
kmeans_model
kmeans_model.re$centers
kmeans_model$centers
kmeans_model$cluster
plot(data1[,1:2],col= kmeans_model$cluster)
plot(data1[,1:2],col= kmeans_model$cluster,title= "clusters")
plot(data1[,1:2],col= kmeans_model$cluster,title= "clusters")
plot(data1[,1:2],col= kmeans_model$cluster,main= "clusters")
kmeans_model$centers
points(kmeans_model$centers[,c(Sepal.Length,Sepal.Width)])
points(kmeans_model$centers[,c("Sepal.Length","Sepal.Width")])
points(kmeans_model$centers[,c("Sepal.Length","Sepal.Width")],col=3)
points(kmeans_model$centers[,c("Sepal.Length","Sepal.Width")],col = 1:3, pch = 8, cex = 3)
## Visualizing clusters
y_kmeans <- kmeans.re$cluster
clusplot(iris_1[, c("Sepal.Length", "Sepal.Width")],
y_kmeans,
lines = 0,
shade = TRUE,
color = TRUE,
labels = 2,
plotchar = FALSE,
span = TRUE,
main = paste("Cluster iris"),
xlab = 'Sepal.Length',
ylab = 'Sepal.Width')
# Installing Packages
install.packages("arules")
install.packages("cluster")
# Loading package
#library(ClusterR)
library(cluster)
#??ClusterR
# Removing initial label of
# Species from original dataset
iris_1 <- iris[, -5]
# Fitting K-Means clustering Model
# to training dataset
set.seed(240) # Setting seed
kmeans.re <- kmeans(iris_1, centers = 3, nstart= 20)
#nstart means initial random number of centroids
#centers means no of clusters
kmeans.re
# Cluster identification for
# each observation
kmeans.re$cluster
kmeans.re$centers
# Confusion Matrix
cm <- table(iris$Species, kmeans.re$cluster)
cm
# Model Evaluation and visualization
#plot(iris_1[c("Sepal.Length", "Sepal.Width")])
#plot(iris_1[c("Sepal.Length", "Sepal.Width")],col = kmeans.re$cluster)
plot(iris_1[c("Sepal.Length", "Sepal.Width")],
col = kmeans.re$cluster,
main = "K-means with 3 clusters")
## Plotiing cluster centers
kmeans.re$centers
kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")]
# cex is font size, pch is symbol
points(kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")],
col = 1:3, pch = 8, cex = 3)
## Visualizing clusters
y_kmeans <- kmeans.re$cluster
clusplot(iris_1[, c("Sepal.Length", "Sepal.Width")],
y_kmeans,
lines = 0,
# cex is font size, pch is symbol
points(kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")],
col = 1:3, pch = 5, cex = 3)
# cex is font size, pch is symbol
points(kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")],
col = 1:3, pch = 8, cex = 3)
# cex is font size, pch is symbol
points(kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")],
col = 1:3, pch = 8, cex = 3)
# Model Evaluation and visualization
#plot(iris_1[c("Sepal.Length", "Sepal.Width")])
#plot(iris_1[c("Sepal.Length", "Sepal.Width")],col = kmeans.re$cluster)
plot(iris_1[c("Sepal.Length", "Sepal.Width")],
col = kmeans.re$cluster,
main = "K-means with 3 clusters")
## Plotiing cluster centers
kmeans.re$centers
kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")]
# cex is font size, pch is symbol
points(kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")],
col = 1:3, pch = 8, cex = 3)
## Visualizing clusters
y_kmeans <- kmeans.re$cluster
iris
iris1=iris
iris1
iris1$Species=NULL
iris1
d = dist(iris1, method = "euclidean")
hfit = hclust(d, method = "average")
plot(hfit)
grps = cutree(hfit,k=4)
grps
rect.hclust(hfit,k=4, border = "red")
install.packages("ROCR")
library(ROCR)
?prediction()
data =  read.csv("D:\\datasets\\social.csv")
sum(is.na(data))
names(data)
View(data)
data$Gender= factor(data$Gender,levels=c("Female","Male"),labels=c(1,2))
#applying the mormalization
data = data[,3:5]
norm = function(x){
return((x-min(x))/(max(x)-min(x)))
}
data$Age =  as.data.frame(lapply(data$Age, norm))
data$EstimatedSalary =as.data.frame(lapply(data$EstimatedSalary,norm))
#spliting the data
library(caTools)
split  = sample.split(data$Age,SplitRatio = 3/4)
train_data = subset(data,split == TRUE)
test_data = subset(data,split == FALSE)
formula = Purchased ~  Age + EstimatedSalary
library(e1071)
svm_model = svm(formula = formula,data= train_data,type = 'C-classification',
kernel = 'linear')
library(caret)
predicted_data = predict(svm_model,test_data)
svm_model = svm(formula = formula,data= train_data,type = 'C-classification',
kernel = 'linear')
library(caret)
predicted_data = predict(svm_model,test_data)
predicted_data
confusionMatrix(table(test_data$Purchased,predicted_data))
prediction(predictions = predicted_data , labels = test_data$Purchased )
#removing the all the precvious data
rm(list = ls())
#loading the data
data = read.csv("D:\\datasets\\Social_Network_Ads_logistic_regression.csv")
#View(data)
head(data)
data$User.ID = NULL
names(data)
#creating the  normmalize function
normalize = function(x){
return((x-max(x))/(max(x)-min(x)))
}
#ploting the age vs salary  before normaliing
library(ggplot2)
data$Age_nom = normalize(data$Age)
data$EstimatedSalary_nom = normalize(data$EstimatedSalary)
ggplot() + geom_line(aes(x= data$Age,data$EstimatedSalary),color = "red")
#ploting the data after normalizing
ggplot() +   geom_line(aes(x= data$Age_nom,y=data$EstimatedSalary_nom),color="blue")
data$Gender = factor(data$Gender,levels = c("Male","Female"),labels = c(1,0))
head(data)
#splitting the data into train and test
set.seed(100)
library(caTools)
split  = sample.split(data$Gender , SplitRatio = 3/4)
train_data = subset(data,split == TRUE)
test_data = subset(data, split == FALSE)
nrow(test_data)
#generalised linear model
names(data)
formula =  Purchased ~ Gender + Age + EstimatedSalary
formula1 = Purchased ~ Gender + Age_nom + EstimatedSalary_nom
logistic_regression_classifier = glm(formula = formula1, data = train_data, family = "binomial")
#predicting the data
predicted_data = predict(logistic_regression_classifier,newdata = test_data,type="response")
predicted_data
length(predicted_data)
logistic_reg_value = function(x){
if(x>=0.5){
return(1)
}else {
return(0)
}
}
predicted_data_func = sapply(predicted_data ,logistic_reg_value )
predicted_data_func
aa = table(predicted_data_func,test_data$Purchased)
#gmodels
library(gmodels)
CrossTable(aa)
#calculating the accuracy
library(caret)
confusionMatrix(aa)
lroc(logistic_regression_classifier)
library(ROCR)
lroc(logistic_regression_classifier)
install.packages("epiDisplay")
library(epiDisplay)
lroc(logistic_regression_classifier)
prediction(predictions = predicted_data , labels = test_data$Purchased )
#calucalating the prediction object
pred = prediction(predictions = predicted_data , labels = test_data$Purchased )
#calculating the roc curve
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
library(ROCR)
#calucalating the prediction object
pred = prediction(predictions = predicted_data , labels = test_data$Purchased )
#calculating the roc curve
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
# Plot the ROC curve
plot(perf, main = "ROC Curve for SVM Model", xlab = "False Positive Rate", ylab = "True Positive Rate")
# Calculate the AUC
auc <- performance(pred, "auc")
auc
auc
hclust(iris[,1:4],method = "euclidian")
plot(hieracherical_cluster)
cluster_no_of_labels = cutree(hieracherical_cluster,k=3)
rm(list = ls())
data = iris
data = iris[,-5]
set.seed(1004)
data[,c(1,2)]
data_dist = dist(data[,c(1,2)])
hieracherical_cluster = hclust(data_dist,method = "average")
hieracherical_cluster$height
plot(hieracherical_cluster)
cluster_no_of_labels = cutree(hieracherical_cluster,k=3)
#k = specify the no of cuts of the tree
cut_boders
rect.hclust(hieracherical_cluster,k=3,border="red")
